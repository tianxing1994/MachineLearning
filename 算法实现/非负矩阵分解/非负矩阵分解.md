
## 非负矩阵分解
NMF(Non-negative matrix factorization), 即对于任意给定的一个非负矩阵 V, 其能够寻找到一个非负矩阵 W 和一个非负矩阵 H, 满足条件 $V = W \cdot H$, 从而将一个非负的矩阵分解为左右两个非负矩阵的乘积. 其中 V 矩阵中每一列代表一个样本, 每一行代表一个特征, W 矩阵称为基矩阵, H 矩阵称为系数矩阵或权重矩阵. 这时用系数矩阵 H 代替原始矩阵, 就可以实现对原矩阵进行降维, 得到数据特片的降维矩阵, 从而减少存储空间.
也可以将 W 看作是对 V 中样本特征的总结, 而 H 则对应地表示将 W 中的特征加权求和为 V 中的每一个样本. 这种方法用于提取样本 V 中数据的特征, 将原样本理解为是由这些特征的组合.
为了找到满足 $V=W \cdot H$ 的 $W$ 和 $H$ 矩阵. 设: 矩阵形状分别为 $V(m, n)$, $W(m, k)$, $H(k, n)$. V 中每一列为一个样本.




### H 的更新
$V$ 中每一列 $V_{j}$ 为一个样本, 所以将 $V=W \cdot H$ 矩阵理解为 $V_{j} = W \cdot H_{j}$, 对每一个 $j \in [0, n)$ 都成立.
采用基于最小二乘法的损失函数如下:
$$\begin{aligned} L(W, H) &= \vline \vline V - W \cdot H \vline \vline^{2} \\ &= \sum_{j=0}^{n-1}\sum_{i=0}^{m-1}[V_{ij} - (W \cdot H)_{ij}]^{2} \\ &= \sum_{j=0}^{n-1} ((V_{j} - WH_{j})^{T} \cdot (V_{j} - WH_{j})) \\ &= \sum_{j=0}^{n-1}((V_{j}^{T} - H_{j}^{T}W^{T}) \cdot (V_{j} - WH_{j})) \\ &= \sum_{j=0}^{n-1}(V_{j}^{T}V_{j} - H_{j}^{T}W^{T}V_{j} - V_{j}^{T}WH_{j} + H_{j}^{T}W^{T}WH_{j}) \end{aligned}$$


损失函数 $L(W, H)$ 中:
$$\begin{aligned} L(W, H_{j}) &= V_{j}^{T}V_{j} - H_{j}^{T}W^{T}V_{j} - V_{j}^{T}WH_{j} + H_{j}^{T}W^{T}WH_{j} \\ \frac{\partial L(W, H_{j})}{\partial H_{j}} &= -2V_{j}^{T}W + 2H_{j}^{T}W^{T}W \end{aligned}$$
$\frac{\partial L(W, H_{j})}{\partial H_{j}}$ 的形状为 $(1, k)$, 1 行, k 列.


损失函数 $L(W, H)$ 对 $H$ 求偏导.
$$\begin{aligned} \frac{\partial L(W, H)}{\partial H} = -2V^{T}W + 2H^{T}W^{T}W \end{aligned}$$
$\frac{\partial L(W, H)}{\partial H}$ 的形状为 $(n, k)$, n 行, k 列.


设学习率为 $\eta$. 则对 $H(k , n)$ 的更新如下:
$$\begin{aligned} H &:= H - \eta \times (-2V^{T}W + 2H^{T}W^{T}W)^{T} \\ &:= H + \eta \times (2V^{T}W - 2H^{T}W^{T}W)^{T} \\ &:= H + 2 \eta \times (W^{T}V - W^{T}WH) \end{aligned}$$
此为加性更新.


以下为乘性更新.
令 (注意, 矩阵没有除法, 这里是对应位置元素相除):
$$\begin{aligned} 2 \eta = \frac{H}{W^{T}WH}, \text{shape=(k, n)} \end{aligned}$$
则有:
$$\begin{aligned} H &:= H + 2 \eta \times (W^{T}V - W^{T}WH) \\ &:= H + \frac{H}{W^{T}WH} \times (W^{T}V - W^{T}WH) \\ &:= H + H \times \frac{W^{T}V}{W^{T}WH} - H \times \frac{W^{T}WH}{W^{T}WH} \\ &:= H \times (1 + \frac{W^{T}V}{W^{T}WH} - 1) \\ &:= H \times \frac{W^{T}V}{W^{T}WH} \end{aligned}$$




### W 的更新
对 $W$ 更新的推导其实和 $H$ 一样, 只是不能很好地理解为样本对样本的转换. 推导如下:
采用基于最小二乘法的损失函数如下:
$$\begin{aligned} L(W, H) &= \vline \vline V - W \cdot H \vline \vline^{2} \\ &= \sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[V_{ij} - (W \cdot H)_{ij}]^{2} \\ &= \sum_{i=0}^{m-1} ((V_{i} - W_{i}H) \cdot (V_{i} - W_{i}H)^{T}) \\ &= \sum_{i=0}^{m-1}((V_{i} - W_{i}H) \cdot (V_{i}^{T} - H^{T}W_{i}^{T})) \\ &= \sum_{i=0}^{m-1}(V_{i}V_{i}^{T} - W_{i}HV_{i}^{T} - V_{i}H^{T}W_{i}^{T} + W_{i}HH^{T}W_{i}^{T}) \end{aligned}$$
损失函数 $L(W, H)$ 中:(1, n), (n, k)
$$\begin{aligned} L(W_{i}, H) &= V_{i}V_{i}^{T} - W_{i}HV_{i}^{T} - V_{i}H^{T}W_{i}^{T} + W_{i}HH^{T}W_{i}^{T} \\ \frac{\partial L(W_{i}, H)}{\partial W_{i}} &= -2HV_{i}^{T} + 2HH^{T}W_{i}^{T} \end{aligned}$$
$\frac{\partial L(W_{i}, H)}{\partial W_{i}}$ 的形状为 $(k, 1)$, k 行, 1 列.


损失函数 $L(W, H)$ 对 $W$ 求偏导.
$$\begin{aligned} \frac{\partial L(W, H)}{\partial W} = -2HV^{T} + 2HH^{T}W^{T} \end{aligned}$$
$\frac{\partial L(W, H)}{\partial H}$ 的形状为 $(k, n)$, k 行, n 列.


设学习率为 $\eta$. 则对 $W(n , k)$ 的更新如下:
$$\begin{aligned} W &:= W - \eta \times (-2HV^{T} + 2HH^{T}W^{T})^{T} \\ &:= W + \eta \times (2HV^{T} - 2HH^{T}W^{T})^{T} \\ &:= W + 2 \eta \times (VH^{T} - WHH^{T}) \end{aligned}$$
此为加性更新.


以下为乘性更新.
令 (注意, 矩阵没有除法, 这里是对应位置元素相除):
$$\begin{aligned} 2 \eta = \frac{W}{WHH^{T}}, \text{shape=(n, k)} \end{aligned}$$
则有:
$$\begin{aligned} W &:= W + 2 \eta \times (VH^{T} - WHH^{T}) \\ &:= W + \frac{W}{WHH^{T}} \times (VH^{T} - WHH^{T}) \\ &:= W + W \times \frac{VH^{T}}{WHH^{T}} - W \times \frac{WHH^{T}}{WHH^{T}} \\ &:= W \times (1 + \frac{VH^{T}}{WHH^{T}} - 1) \\ &:= W \times \frac{VH^{T}}{WHH^{T}} \end{aligned}$$




练习实例:

```python
"""
参考链接:
https://www.cnblogs.com/gavanwanggw/p/7337227.html
"""
import numpy as np


def nmf(v, n_topics, max_iters=100, epsilon=1e-5):
k = n_topics
m, n = np.shape(v)
w = np.array(np.random.random((m, k)))
h = np.array(np.random.random((k, n)))

for i in range(max_iters):
v_pred = np.dot(w, h)
loss = np.sum(np.power(v_pred - v, 2))

# print(f"iter: {i}, loss: {loss}")
if loss < epsilon:
break

# 乘性更新
matrix_h = np.dot(w.T, v) / np.dot(np.dot(w.T, w), h)
h = h * matrix_h

matrix_w = np.dot(v, h.T) / np.dot(np.dot(w, h), h.T)
w = w * matrix_w

return w, h


if __name__ == '__main__':
w0 = np.array([[1, 0],
[0, 1]])
h0 = np.array([[0.3, 0.4, 0.5, 0.6, 0.7, 0.7, 0.1, 0.1, 0.2, 0.1],
[0.4, 0.3, 0.2, 0.1, 0.1, 0.2, 0.5, 0.6, 0.2, 0.8]])
v0 = np.dot(w0, h0)
# print(v0)
w1, h1 = nmf(v0, 2, max_iters=1000, epsilon=1e-9)
v1 = np.dot(w1, h1)
print(w1)
# print(h1)
# print(v1)
```



