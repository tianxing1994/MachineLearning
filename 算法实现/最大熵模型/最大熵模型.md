## 最大熵模型



### 贝叶斯

设有一组样本 $(X, Y)$, 其中不重复的 $X$ 有 $(X_{1}, X_{2}, \cdot \cdot \cdot X_{n})$, 不重复的 $Y$ 有 $(Y_{1}, Y_{2}, \cdot \cdot \cdot Y_{m})$. 

可以用 $P(X_{i}, Y_{j})$ 表示任意 $X_{i}, Y_{j}$ 同时发生的概率. $X_{i} \in (X_{1}, X_{2}, \cdot \cdot \cdot X_{n})$, $Y_{j} \in (Y_{1}, Y_{2}, \cdot \cdot \cdot Y_{m})$. 且 $P(X_{i}, Y_{j})$ 可以根据既有的样本 $(X, Y)$ 计算得出. 

假设, 我们从 $X_{i}$ 计算出三个特征 $(X_{i1}, X_{i2}, X_{i3})$ 用来描述 $X_{i}$, 则根据既有样本统计可以计算 $P(X_{i1}, Y_{j})$ 的值, 且有: $P(X_{i1}, Y_{j}) + P(X_{i2}, Y_{j}) + P(X_{i3}, Y_{j}) = P(X, Y_{j})$, 

据此, 当出现样本 $\hat{X} = (\hat{X_{1}}, \hat{X_{2}}, \hat{X_{3}})$ 时, 从样本统计的先验中得出各 $P(\hat{X_{k}}, Y_{j}), k \in (1, 2, 3)$ 的值, 有:

$$\begin{aligned} P(\hat{X}, Y_{j}) = P(\hat{X_{1}}, Y_{j}) + P(\hat{X_{2}}, Y_{j}) + P(\hat{X_{3}}, Y_{j})  \end{aligned}$$

以上是贝叶斯方法. 



### 最大熵

但贝叶斯认为: $X_{i}$ 中的三个特征 $(X_{i1}, X_{i2}, X_{i3})$ 应不相关. 而当特征相关时, 则其本身是有缺陷的. 

为此, 建立一个新的模型, 此模型需要自己计算 $P(X_{1}, Y_{j}), P(X_{2}, Y_{j}), P(X_{3}, Y_{j})$, 而不是从样本统计的先验中得出这些值. 但要求, 它们应满足: 

$P(X_{i1}, Y_{j}) + P(X_{i2}, Y_{j}) + P(X_{i3}, Y_{j}) = P(X_{i}, Y_{j})$

其中 $P(X_{i} , Y_{j})$ 是从样本统计中得出的. 

对于任一样本, $P(X_{i} , Y_{j})$ 是确定的. 但上式中 $P(X_{k}, Y_{j}), k \in (1, 2, 3)$ 却可以有无数种可能. 

此时应用最大熵原理. 即要求下式中, 熵应尽可能大: 

$$\begin{aligned} H = P(X_{i1}, Y_{j}) \times log(\frac{1}{P(X_{i1}, Y_{j})}) + P(X_{i2}, Y_{j}) \times log(\frac{1}{P(X_{i2}, Y_{j})}) + P(X_{i3}, Y_{j}) \times log(\frac{1}{P(X_{i3}, Y_{j})}) \end{aligned}$$

上式变换: 

$$\begin{aligned} H &= P(X_{i1}, Y_{j}) \times log(\frac{1}{P(X_{i1}, Y_{j})}) + P(X_{i2}, Y_{j}) \times log(\frac{1}{P(X_{i2}, Y_{j})}) + P(X_{i3}, Y_{j}) \times log(\frac{1}{P(X_{i3}, Y_{j})}) \\ &= - P(X_{i1}, Y_{j}) \times log(P(X_{i1}, Y_{j})) - P(X_{i2}, Y_{j}) \times log(P(X_{i2}, Y_{j})) - P(X_{i3}, Y_{j}) \times log(P(X_{i3}, Y_{j})) \\  \hat{H} &= P(X_{i1}, Y_{j}) \times log(P(X_{i1}, Y_{j})) + P(X_{i2}, Y_{j}) \times log(P(X_{i2}, Y_{j})) + P(X_{i3}, Y_{j}) \times log(P(X_{i3}, Y_{j})) \end{aligned}$$

变成求 $\hat{H}$ 的最小值. 



问题变成, 在给定条件下, 求方程的最小值: 

$$\begin{aligned} & P(X_{i1}, Y_{j}) + P(X_{i2}, Y_{j}) + P(X_{i3}, Y_{j}) = P(X_{i}, Y_{j}) \\ & \hat{H} = P(X_{i1}, Y_{j}) \times log(P(X_{i1}, Y_{j})) + P(X_{i2}, Y_{j}) \times log(P(X_{i2}, Y_{j})) + P(X_{i3}, Y_{j}) \times log(P(X_{i3}, Y_{j})) \end{aligned}$$



#### 拉格朗日乘数法

在条件: $\phi(x, y) = 0$ 下, 求 $z = f(x, y)$ 的极值. 

$$\begin{aligned} & L(x, y) = f(x, y) + \lambda \phi(x, y) \end{aligned}$$



$$\begin{aligned} L(P_{1}, P_{2}, P_{3}) &= P_{1} \times log(P_{1}) + P_{2} \times log(P_{2}) + P_{3} \times log(P_{3}) + \lambda (P_{1} + P_{2} + P_{3} - P)  \\ \end{aligned}$$

其中: $P_{1} = P(X_{i1}, Y_{j}), P_{2} = P(X_{i2}, Y_{j}), P_{3} = P(X_{i3}, Y_{j}), P = P(X_{i}, Y_{j})$



求: $L(P_{1}, P_{2}, P_{3})$ 的最小值. 





#### 梯度下降求最大熵

上面的推导过程存在的问题是, 我们的公式只应用了一组样本. 这样, 对特定的一组样本求出的解对整个数据集是没有意义的. 所以, 采用梯度下降法来求解. 即: 每次应用一组样本, 使 $P(X_{i1}, Y_{j}), P(X_{i2}, Y_{j}), P(X_{i3}, Y_{j})$ 的值向目标函数的最小值迭代. 

梯度方向如下: 

$$\begin{aligned} \frac{\partial L}{\partial P_{k}} = logP_{k} + \lambda, k \in (1, 2, 3) \end{aligned}$$



#### 导数表

$$\begin{aligned} y &= log_{a}x  & y^{'} = \frac{1}{x lna} \\ y &= lnx  & y^{'} = \frac{1}{x} \end{aligned}$$





















